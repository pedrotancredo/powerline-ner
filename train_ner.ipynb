{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Lrsl9ktUvaj4Jv9_Lpubw1zg0xx8HvHV","timestamp":1705150939848},{"file_id":"1BSywHgWKzwODXCW8nipxJMfi8vfbeg4m","timestamp":1702050181584},{"file_id":"https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb","timestamp":1701955152565}],"gpuType":"T4","collapsed_sections":["bmN3H7jbV7cP","e7wfLWyYkvDi","UN9cKO5_5mrj","mVXD7VBTWxjl","EwvaWgPVXwVx","r4jcSOJr680a","wQM6ee8AX6pC","MnuBc4shXn97","sqDklprSqB5d"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MyETdB-dkBsX"},"source":["# **Fine-tuning de modelo BERT para reconhecimento de entidade nomeada NER**"]},{"cell_type":"markdown","source":["## Introdução"],"metadata":{"id":"bmN3H7jbV7cP"}},{"cell_type":"markdown","source":["Este notebook, utiliza-se a classe **BertForTokenClassification**, incluída no módulo [Transformers](https://github.com/huggingface/transformers) da HuggingFace. Este modelo tem o BERT como sua arquitetura base incluída uma camada de classificação de token, que possibilita fazer previsões a nível de tokens, estratégia normalmente utilizada para reconhecimento de entidades nomeadas, ao invés de sequencias."],"metadata":{"id":"UzpEJfK5Wjay"}},{"cell_type":"markdown","metadata":{"id":"e7wfLWyYkvDi"},"source":["## Preparação do Ambiente"]},{"cell_type":"markdown","source":["Para utilização deste notebook é necessário ter instalado no ambiente os seguintes módulos python:\n","\n","* [pandas](https://pypi.org/project/pandas/)\n","* [numpy](https://pypi.org/project/numpy/)\n","* [sklearn](https://pypi.org/project/scikit-learn/)\n","* [pytorch](https://pypi.org/project/torch/)\n","* [transformers](https://pypi.org/project/transformers/)\n","* [seqeval](https://pypi.org/project/seqeval/)\n","\n","Por padrão o ambiente do Google Colab já possui a maior parte destes módulos instalados por padrão com exceção do transformers e do seqeval (GPU version):"],"metadata":{"id":"RUXBj_4t50XB"}},{"cell_type":"code","metadata":{"id":"d4_YJqjR_Gjw"},"source":["!pip install transformers seqeval[gpu]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inclusão dos módulos"],"metadata":{"id":"id-bfnoP4QG-"}},{"cell_type":"code","metadata":{"id":"IEnlUbgm8z3B"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jzq1w3L1K5M-"},"source":["Como os algoritmos de Deep Learning podem ser acelerados significativamente quando realizados com processamento utilizando uma GPU em vez de uma CPU, é necessário certificar-se de que ao executar este Notebook a opção por GPU esteja ativada (Verifique \"Runtime\" - \"Change runtime type\" - e defina o acelerador de hardware para \"GPU\")."]},{"cell_type":"markdown","source":["Podemos definir o dispositivo padrão como GPU usando o seguinte código (se imprimir \"cuda\", significa que a GPU foi reconhecida):"],"metadata":{"id":"s_6XgK2F5Px3"}},{"cell_type":"code","metadata":{"id":"Sm1krxJtKxpx","tags":[]},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamento e pré-processamento de dados"],"metadata":{"id":"UN9cKO5_5mrj"}},{"cell_type":"markdown","metadata":{"id":"ahwMsmyG5ZPE"},"source":["O Reconhecimento de Entidades Nomeadas (NER) utiliza um esquema de anotação específico, que é definido ao nível da palavra. Um formato bastante popular utilizado para este tipo de aplicação é o [IOB-tagging](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), que significa Inside-Outside-Beginning.\n","\n","Neste formato cada tag indica se a palavra correspondente está **dentro**, **fora** ou no **início** de uma entidade nomeada específica. A razão para isso é que entidades nomeadas podem ser formadas por mais de uma palavra.\n","\n","Por exemplo, se você tiver uma frase como \"Barack Obama nasceu no Havaí\", então as etiquetas correspondentes seriam [B-PERS, I-PERS, O, O, B-GEO]. B-PERS\n","significa que a palavra \"Barack\" é o início de uma pessoa, I-PERS significa que\n","a palavra \"Obama\" está dentro de uma pessoa, \"O\" significa que a palavra \"nasceu\" está fora de uma entidade nomeada, e assim por diante. Portanto, geralmente temos tantas etiquetas quanto palavras em uma frase.\n","\n","Assim, ao preparar dados para treinar um modelo de Deep Learning para NER, é necessário ter seus dados neste formato IOB (ou formatos semelhantes, como [BILOU](https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition)).\n","\n","Abaixo segue indicação de algumas ferramentas que auxiliam o processo de anotação:\n","\n","- [Prodigy](https://prodi.gy/)\n","- [Tagtog](https://docs.tagtog.com/)\n","- [Doccano](https://doccano.herokuapp.com/)\n","- [Label Studio](https://labelstud.io/)\n","\n","Embora a utilização dessas ferramentas gere um arquivo estruturado, pode ser necessário realizar algum pré-processamento nos dados para ajustar a saída ao formato específico necessário para o treinamento."]},{"cell_type":"code","metadata":{"id":"deLB9HVX5I6F"},"source":["data = pd.read_csv(\"colab.csv\", encoding='utf-8', sep=';')\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucYjhq6uRAmY"},"source":["Vamos verificar quantas sentenças e palavras (e suas respectivas etiquetas) existem neste conjunto de dados:"]},{"cell_type":"code","metadata":{"id":"6gMibEJXTKDw"},"source":["data.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUGXZOfE_GkO"},"source":["Como podemos ver, há 2.206 sentenças no dataset, totalizando mais de 20 mil palavras e etiquetas. Isso corresponde a uma média de 9 palavras por frase.\n","\n","Vamos dar uma olhada nas diferentes classes e em sua frequência:"]},{"cell_type":"code","metadata":{"tags":[],"id":"s4Jn1fVT_GkO"},"source":["print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","frequencies = data.Tag.value_counts()\n","frequencies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Com a retirada do sufixo das etiquetas é possível chegar a seguinte distribuição:"],"metadata":{"id":"SMmC53a4BQl_"}},{"cell_type":"code","source":["# Cria uma cópia da coluna 'Tag' column\n","tags = data['Tag'].copy()\n","\n","# Divide as tags considerando o texto a partir do separador \"-\"\n","tags = tags.apply(lambda x: x.split('-')[1] if '-' in x else x)\n","\n","# Conta as ocorrências de cada tag\n","frequencies = tags.value_counts()\n","print(frequencies)"],"metadata":{"id":"IP74kOIBEtvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGUQemBz_GkV"},"source":["Na base utilizada neste projeto existem poucas representações da entidade SUBESTACAO, além disso, esta entidade normalmente aparece com sobreposição em relação a entidade LINHA. Devido a isso foi necessária sua exclusão da base, visto que sua presença pode prejudicar o treinamento das demais classes."]},{"cell_type":"code","metadata":{"id":"8iorLrU4_GkW"},"source":["entities_to_remove = [\"B-SUBESTACAO\", \"I-SUBESTACAO\"]\n","data = data[~data.Tag.isin(entities_to_remove)]\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mskU4h0oRKEF"},"source":["Nas próximas células é feita o ajuste para agrupar as palavras em suas sentenças:"]},{"cell_type":"code","metadata":{"id":"zkW2vNcO-uMH"},"source":["# O pandas possui uma função muito útil chamada \"forward fill\" para preencher valores ausentes com base no último valor não-NaN superior:\n","data = data.fillna(method='ffill')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hmd-ow389k6Y"},"source":["# Cria uma nova coluna chamada \"sentence\" que agrupa as palavras por frase\n","data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","\n","# Cria uma nova coluna chamada \"word_labels\" que agrupa as etiquetas por frase\n","data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JsjhdQbE-Lve"},"source":["Para possibilitar o treinamento e interpretação dos resultados dois dicionários são criados: um que mapeia tags para índices e outro que mapeia índices para suas tags.\n","\n","Isso é necessário na criação dos rótulos (já que os computadores trabalham com números (índices), em vez de palavras (etiquetas)).\n"]},{"cell_type":"code","metadata":{"id":"CFRDM8WsQXvL"},"source":["label2id = {k: v for v, k in enumerate(data.Tag.unique())}\n","id2label = {v: k for v, k in enumerate(data.Tag.unique())}\n","label2id"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J08Cvk_USgbM"},"source":["No exemplo podem ser observados 13 tags únicas.\n","\n","Apenas as colunas \"sentence\" e \"word_labels\" serão utilizadas, além disso as duplicatas devem ser removidas:"]},{"cell_type":"code","metadata":{"id":"SrEgd4PZUgmF"},"source":["data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3ArUiVRqw0C"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8obZumRTBrT"},"source":["Let's verify that a random sentence and its corresponding tags are correct:"]},{"cell_type":"code","metadata":{"id":"eUvupomW_fbe"},"source":["data.iloc[41].sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dLyY3Oi_lvp"},"source":["data.iloc[41].word_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Distribuição de comprimento de sentenças\n","\n","fig, ax = plt.subplots(figsize=(10, 5))\n","ax.hist([len(x.split(',')) for x in data.word_labels], bins=15)\n","ax.set_xlabel('Número de palavras em uma sentença')\n","ax.set_ylabel('Quantidade')\n","plt.show()\n"],"metadata":{"id":"XxnuHcPGDsRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: need a df showing the perncentile of data included if considered sentences that have the lenght smaller or equal than: 2,4,8,16,32,64 words\n","\n","data['word_length'] = data['sentence'].apply(lambda x: len(x.split()))\n","data\n","data.word_length.describe()\n","threshold = [2,4,8,16,32,64]\n","quantiles = [data.word_length <= x for x in threshold]\n","for i in range(len(quantiles)):\n","  print('Sentenças com {} palavras ou menos: {}%'.format(threshold[i], round(quantiles[i].mean()*100, 2)))\n"],"metadata":{"id":"tT8X4vVKJHqF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definição e Treinamento do Modelo"],"metadata":{"id":"mVXD7VBTWxjl"}},{"cell_type":"code","source":["# @title Parâmetros para Treino\n","MAX_LEN = 64 # @param [2, 4, 8, 16, 32, 64, 128] {type:\"raw\"}\n","TRAIN_BATCH_SIZE = 16 # @param [2, 4, 8, 16, 32] {type:\"raw\"}\n","VALID_BATCH_SIZE = 16 # @param [2, 4, 8, 16, 32] {type:\"raw\"}\n","EPOCHS = 6 # @param {type:\"slider\", min:1, max:15, step:1}\n","LEARNING_RATE = 5e-05 # @param {type:\"number\"}\n","MAX_GRAD_NORM = 10 # @param {type:\"integer\"}\n","MODEL = \"neuralmind/bert-base-portuguese-cased\" # @param [\"neuralmind/bert-base-portuguese-cased\", \"neuralmind/bert-large-portuguese-cased\"] {allow-input: true}\n","tokenizer = BertTokenizer.from_pretrained(MODEL)"],"metadata":{"cellView":"form","id":"g3QiSNPEZCD4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Treinamento"],"metadata":{"id":"Ik5yLVdGn1WC"}},{"cell_type":"markdown","metadata":{"id":"f5EHpuB78pIa"},"source":["#### **Preparing the dataset and dataloader**"]},{"cell_type":"markdown","metadata":{"id":"15x7zmZnTgFx"},"source":["Now that our data is preprocessed, we can turn it into PyTorch tensors such that we can provide it to the model. Let's start by defining some key variables that will be used later on in the training/evaluation process:"]},{"cell_type":"markdown","metadata":{"id":"wPYV2Ld6Tr5I"},"source":["A tricky part of NER with BERT is that BERT relies on **wordpiece tokenization**, rather than word tokenization. This means that we should also define the labels at the wordpiece-level, rather than the word-level!\n","\n","For example, if you have word like \"Washington\" which is labeled as \"b-gpe\", but it gets tokenized to \"Wash\", \"##ing\", \"##ton\", then we will have to propagate the word’s original label to all of its wordpieces: \"b-gpe\", \"b-gpe\", \"b-gpe\". The model should be able to produce the correct labels for each individual wordpiece. The function below (taken from [here](https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L118)) implements this.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RNzSgZTfGUd8"},"source":["def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n","    \"\"\"\n","    Word piece tokenization makes it difficult to match word labels\n","    back up with individual word pieces. This function tokenizes each\n","    word one at a time so that it is easier to preserve the correct\n","    label for each subword. It is, of course, a bit slower in processing\n","    time, but it will help our model achieve higher accuracy.\n","    \"\"\"\n","\n","    tokenized_sentence = []\n","    labels = []\n","\n","    sentence = sentence.strip()\n","\n","    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ez7qlFHl56ZW"},"source":["Note that this is a **design decision**. You could also decide to only label the first wordpiece of each word and let the model only learn this (this is what was done in the original BERT paper, see Github discussion [here](https://github.com/huggingface/transformers/issues/64#issuecomment-443703063)). Another design decision could be to give the first wordpiece of each word the original word label, and then use the label “X” for all subsequent subwords of that word.\n","\n","All of them lead to good performance.\n","\n","Next, we define a regular PyTorch [dataset class](https://pytorch.org/docs/stable/data.html) (which transforms examples of a dataframe to PyTorch tensors). Here, each sentence gets tokenized, the special tokens that BERT expects are added, the tokens are padded or truncated based on the max length of the model, the attention mask is created and the labels are created based on the dictionary which we defined above.\n","\n","For more information about BERT's inputs, see [here](https://huggingface.co/transformers/glossary.html).  "]},{"cell_type":"code","metadata":{"id":"aJty_Abw8_xK"},"source":["class dataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, index):\n","        # step 1: tokenize (and adapt corresponding labels)\n","        sentence = self.data.sentence[index]\n","        word_labels = self.data.word_labels[index]\n","        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n","\n","        # step 2: add special tokens (and corresponding labels)\n","        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n","        labels.insert(0, \"O\") # add outside label for [CLS] token\n","        labels.insert(-1, \"O\") # add outside label for [SEP] token\n","\n","        # step 3: truncating/padding\n","        maxlen = self.max_len\n","\n","        if (len(tokenized_sentence) > maxlen):\n","          # truncate\n","          tokenized_sentence = tokenized_sentence[:maxlen]\n","          labels = labels[:maxlen]\n","        else:\n","          # pad\n","          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n","\n","        # step 4: obtain the attention mask\n","        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","\n","        # step 5: convert tokens to input ids\n","        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","\n","        label_ids = [label2id[label] for label in labels]\n","        # the following line is deprecated\n","        #label_ids = [label if label != 0 else -100 for label in label_ids]\n","\n","        return {\n","              'ids': torch.tensor(ids, dtype=torch.long),\n","              'mask': torch.tensor(attn_mask, dtype=torch.long),\n","              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","              'targets': torch.tensor(label_ids, dtype=torch.long)\n","        }\n","\n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTP7zuWGWGUd"},"source":["Now, based on the class we defined above, we can create 2 datasets, one for training and one for testing. Let's use a 80/20 split:"]},{"cell_type":"code","metadata":{"id":"jrkdZBLYHVcB","tags":[]},"source":["train_size = 0.8\n","train_dataset = data.sample(frac=train_size,random_state=200)\n","test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(data.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n","testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ptv5AT_iTb7W"},"source":["Let's have a look at the first training example:"]},{"cell_type":"code","metadata":{"id":"phmPylgAm8Xy"},"source":["training_set[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvU4nzL2W2Xo"},"source":["Let's verify that the input ids and corresponding targets are correct:"]},{"cell_type":"code","source":["training_set[0][\"ids\"]"],"metadata":{"id":"ZHoufyakY18x"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWgnNJrYW2GP","tags":[]},"source":["# print the first 30 tokens and corresponding labels\n","for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n","  print('{0:10}  {1}'.format(token, id2label[label.item()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ky68FcTgWnfN"},"source":["Now, let's define the corresponding PyTorch dataloaders:"]},{"cell_type":"code","metadata":{"id":"KIw793myWOmi"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73OzU7oXRxR8"},"source":["#### **Defining the model**"]},{"cell_type":"markdown","metadata":{"id":"T-iGhnhdLNdP"},"source":["Here we define the model, BertForTokenClassification, and load it with the pretrained weights of \"bert-base-uncased\". The only thing we need to additionally specify is the number of labels (as this will determine the architecture of the classification head).\n","\n","Note that only the base layers are initialized with the pretrained weights. The token classification head of top has just randomly initialized weights, which we will train, together with the pretrained weights, using our labelled dataset. This is also printed as a warning when you run the code cell below.\n","\n","Then, we move the model to the GPU."]},{"cell_type":"code","metadata":{"id":"cB9MR3KcWXUs","tags":[]},"source":["model = BertForTokenClassification.from_pretrained(MODEL,\n","                                                   num_labels=len(id2label),\n","                                                   id2label=id2label,\n","                                                   label2id=label2id)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pp7Yl4JyWhDj"},"source":["#### **Training the model**\n","\n","Before training the model, let's perform a sanity check, which I learned thanks to Andrej Karpathy's wonderful [cs231n course](http://cs231n.stanford.edu/) at Stanford (see also his [blog post about debugging neural networks](http://karpathy.github.io/2019/04/25/recipe/)). The initial loss of your model should be close to -ln(1/number of classes) = -ln(1/17) = 2.83.\n","\n","Why? Because we are using cross entropy loss. The cross entropy loss is defined as -ln(probability score of the model for the correct class). In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/17. The loss for a given token will thus be -ln(1/17). As PyTorch's [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (which is used by `BertForTokenClassification`) uses *mean reduction* by default, it will compute the mean loss for each of the tokens in the sequence (in other words, for all of the 512 tokens). The mean of 512 times -log(1/17) is, you guessed it, -log(1/17).  \n","\n","Let's verify this:\n","\n"]},{"cell_type":"code","metadata":{"id":"eqAN7YVIjKTr"},"source":["ids = training_set[0][\"ids\"].unsqueeze(0)\n","mask = training_set[0][\"mask\"].unsqueeze(0)\n","targets = training_set[0][\"targets\"].unsqueeze(0)\n","ids = ids.to(device)\n","mask = mask.to(device)\n","targets = targets.to(device)\n","outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","initial_loss = outputs[0]\n","initial_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLdwsru9Mh7U"},"source":["This looks good. Let's also verify that the logits of the neural network have a shape of (batch_size, sequence_length, num_labels):"]},{"cell_type":"code","metadata":{"id":"X-z6YCpGnvfj"},"source":["tr_logits = outputs[1]\n","tr_logits.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwDLXxOVOCvD"},"source":["Next, we define the optimizer. Here, we are just going to use Adam with a default learning rate. One can also decide to use more advanced ones such as AdamW (Adam with weight decay fix), which is [included](https://huggingface.co/transformers/main_classes/optimizer_schedules.html) in the Transformers repository, and a learning rate scheduler, but we are not going to do that here."]},{"cell_type":"code","metadata":{"id":"kznSQfGIWdU4"},"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZQ8JMF0NOe1"},"source":["Now let's define a regular PyTorch training function. It is partly based on [a really good repository about multilingual NER](https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L344)."]},{"cell_type":"code","metadata":{"id":"GLFivpkwW1HY"},"source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","\n","    for idx, batch in enumerate(training_loader):\n","\n","        ids = batch['ids'].to(device, dtype = torch.long)\n","        mask = batch['mask'].to(device, dtype = torch.long)\n","        targets = batch['targets'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","        loss, tr_logits = outputs.loss, outputs.logits\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += targets.size(0)\n","\n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","\n","        # compute training accuracy\n","        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","        targets = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","        tr_preds.extend(predictions)\n","        tr_labels.extend(targets)\n","\n","        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","\n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2dsCyP7dcF3"},"source":["And let's train the model!"]},{"cell_type":"code","metadata":{"id":"y07Ybw8rZeZ7","tags":[]},"source":["for epoch in range(EPOCHS):\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Resultados"],"metadata":{"id":"EwvaWgPVXwVx"}},{"cell_type":"markdown","metadata":{"id":"r4jcSOJr680a"},"source":["#### **Evaluating the model**"]},{"cell_type":"markdown","metadata":{"id":"rYUTuOEUdfFJ"},"source":["Now that we've trained our model, we can evaluate its performance on the held-out test set (which is 20% of the data). Note that here, no gradient updates are performed, the model just outputs its logits."]},{"cell_type":"code","metadata":{"id":"RIVVfFHi7Aw7"},"source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","\n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","\n","            ids = batch['ids'].to(device, dtype = torch.long)\n","            mask = batch['mask'].to(device, dtype = torch.long)\n","            targets = batch['targets'].to(device, dtype = torch.long)\n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","            loss, eval_logits = outputs.loss, outputs.logits\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += targets.size(0)\n","\n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","\n","            # compute evaluation accuracy\n","            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","            targets = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","            eval_labels.extend(targets)\n","            eval_preds.extend(predictions)\n","\n","            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    #print(eval_labels)\n","    #print(eval_preds)\n","\n","    labels = [id2label[id.item()] for id in eval_labels]\n","    predictions = [id2label[id.item()] for id in eval_preds]\n","\n","    #print(labels)\n","    #print(predictions)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")\n","\n","    return labels, predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJaONluRdq-e"},"source":["As we can see below, performance is quite good! Accuracy on the test test is > 93%."]},{"cell_type":"code","metadata":{"id":"2BrxRjvxApY8","tags":[]},"source":["labels, predictions = valid(model, testing_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SAznLDwx_U2X"},"source":["However, the accuracy metric is misleading, as a lot of labels are \"outside\" (O), even after omitting predictions on the [PAD] tokens. What is important is looking at the precision, recall and f1-score of the individual tags. For this, we use the seqeval Python library:"]},{"cell_type":"markdown","source":["### Saída"],"metadata":{"id":"wQM6ee8AX6pC"}},{"cell_type":"code","metadata":{"id":"0jDNXrjr-6BW","tags":[]},"source":["from seqeval.metrics import classification_report\n","\n","print(classification_report([labels], [predictions]))\n","\n","print(\"MODEL:\",MODEL)\n","print(\"MAX_LEN:\",MAX_LEN)\n","print(\"TRAIN_BATCH_SIZE:\",TRAIN_BATCH_SIZE)\n","print(\"VALID_BATCH_SIZE:\",VALID_BATCH_SIZE)\n","print(\"EPOCHS:\",EPOCHS)\n","print(\"LEARNING_RATE:\",LEARNING_RATE)\n","print(\"MAX_GRAD_NORM:\",MAX_GRAD_NORM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inferência"],"metadata":{"id":"MnuBc4shXn97"}},{"cell_type":"code","metadata":{"id":"zPDla1mmZiax","tags":[]},"source":["sentence = \"inspeção detalhada na torre 37 da lt adrianópolis cachoeira paulista, isolador quebrado na fase lateral esquerda\"\n","\n","inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n","\n","# move to gpu\n","ids = inputs[\"input_ids\"].to(device)\n","mask = inputs[\"attention_mask\"].to(device)\n","# forward pass\n","outputs = model(ids, mask)\n","logits = outputs[0]\n","\n","active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n","wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","word_level_predictions = []\n","for pair in wp_preds:\n","  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n","    # skip prediction\n","    continue\n","  else:\n","    word_level_predictions.append(pair[1])\n","\n","# we join tokens, if they are not special ones\n","str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n","print(str_rep)\n","print(word_level_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n","# pipe(\"inspeção detalhada na torre 37 da ibntpr1, isolador quebrado na fase lateral esquerda\")\n","pipe(\"cordoalha rompida no para-raios esquerdo da torre 233 na linha itumbiara porto colombia 1\")"],"metadata":{"id":"D5KB5TKRcdRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqDklprSqB5d"},"source":["## **Salvando o Modelo**"]},{"cell_type":"code","source":["model.save_pretrained('base_m3')"],"metadata":{"id":"r8Vqj1e-4HOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.save_pretrained('base_m3')"],"metadata":{"id":"IG2-tVhp4OYf"},"execution_count":null,"outputs":[]}]}